{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CIFAR-10 Image Classification with ResNet18\n",
        "\n",
        "## Project Overview\n",
        "This notebook implements a deep learning model for image classification on the CIFAR-10 dataset using ResNet18 architecture. The project includes:\n",
        "\n",
        "- **Dataset**: CIFAR-10 (60,000 color images across 10 object categories)\n",
        "- **Model**: ResNet18 with residual connections to address vanishing gradient problem\n",
        "- **Framework**: PyTorch with data augmentation and preprocessing\n",
        "- **Evaluation**: Accuracy metrics, learning curves, and misclassified examples\n",
        "- **Comparison**: ResNet18 vs simple CNN baseline\n",
        "\n",
        "## Table of Contents\n",
        "1. [Setup and Imports](#setup)\n",
        "2. [Data Loading and Preprocessing](#data)\n",
        "3. [Model Architectures](#models)\n",
        "4. [Training Setup](#training)\n",
        "5. [Model Training](#train)\n",
        "6. [Evaluation and Visualization](#evaluation)\n",
        "7. [Results Comparison](#comparison)\n",
        "8. [Conclusion](#conclusion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports {#setup}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import time\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set matplotlib style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Preprocessing {#data}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading CIFAR-10 dataset...\n",
            "Training samples: 50000\n",
            "Test samples: 10000\n",
            "Number of classes: 10\n",
            "Batch size: 128\n",
            "\n",
            "Sample training images:\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 8 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# CIFAR-10 class names\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Data preprocessing and augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "print(\"Loading CIFAR-10 dataset...\")\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "print(f\"Number of classes: {len(class_names)}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "# Display some sample images\n",
        "def show_samples(dataset, num_samples=8):\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i in range(num_samples):\n",
        "        image, label = dataset[i]\n",
        "        # Denormalize for display\n",
        "        image = image * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1) + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
        "        image = torch.clamp(image, 0, 1)\n",
        "        \n",
        "        axes[i].imshow(image.permute(1, 2, 0))\n",
        "        axes[i].set_title(f'{class_names[label]}')\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nSample training images:\")\n",
        "show_samples(train_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Architectures {#models}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet18 Architecture:\n",
            "ResNet18(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "ResNet18 Parameters: 11,173,962\n",
            "\n",
            "==================================================\n",
            "Simple CNN Architecture:\n",
            "SimpleCNN(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=2048, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "\n",
            "Simple CNN Parameters: 620,362\n",
            "\n",
            "Parameter comparison:\n",
            "ResNet18: 11,173,962 parameters\n",
            "Simple CNN: 620,362 parameters\n",
            "ResNet18 has 18.0x more parameters than Simple CNN\n"
          ]
        }
      ],
      "source": [
        "# ResNet18 Implementation\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    \n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        \n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.in_planes = 64\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
        "        self.linear = nn.Linear(512 * BasicBlock.expansion, num_classes)\n",
        "        \n",
        "    def _make_layer(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * BasicBlock.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Simple CNN Baseline\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create model instances\n",
        "resnet18 = ResNet18(num_classes=10).to(device)\n",
        "simple_cnn = SimpleCNN(num_classes=10).to(device)\n",
        "\n",
        "# Print model architectures\n",
        "print(\"ResNet18 Architecture:\")\n",
        "print(resnet18)\n",
        "print(f\"\\nResNet18 Parameters: {sum(p.numel() for p in resnet18.parameters()):,}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Simple CNN Architecture:\")\n",
        "print(simple_cnn)\n",
        "print(f\"\\nSimple CNN Parameters: {sum(p.numel() for p in simple_cnn.parameters()):,}\")\n",
        "\n",
        "# Model size comparison\n",
        "resnet_params = sum(p.numel() for p in resnet18.parameters())\n",
        "cnn_params = sum(p.numel() for p in simple_cnn.parameters())\n",
        "print(f\"\\nParameter comparison:\")\n",
        "print(f\"ResNet18: {resnet_params:,} parameters\")\n",
        "print(f\"Simple CNN: {cnn_params:,} parameters\")\n",
        "print(f\"ResNet18 has {resnet_params/cnn_params:.1f}x more parameters than Simple CNN\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training Setup {#training}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training setup completed!\n",
            "Number of epochs: 50\n",
            "Learning rate: 0.1\n",
            "Weight decay: 0.0001\n",
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Training configuration\n",
        "num_epochs = 50\n",
        "learning_rate = 0.1\n",
        "weight_decay = 1e-4\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizers\n",
        "resnet_optimizer = optim.SGD(resnet18.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
        "cnn_optimizer = optim.SGD(simple_cnn.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
        "\n",
        "# Learning rate schedulers\n",
        "resnet_scheduler = optim.lr_scheduler.MultiStepLR(resnet_optimizer, milestones=[30, 40], gamma=0.1)\n",
        "cnn_scheduler = optim.lr_scheduler.MultiStepLR(cnn_optimizer, milestones=[30, 40], gamma=0.1)\n",
        "\n",
        "# Training and validation functions\n",
        "def train_model(model, optimizer, scheduler, train_loader, test_loader, num_epochs, model_name):\n",
        "    \"\"\"Train a model and return training history\"\"\"\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "    \n",
        "    print(f\"Training {model_name}...\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "            \n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}/{len(train_loader)}, '\n",
        "                      f'Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n",
        "        \n",
        "        # Calculate training metrics\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100. * correct / total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                test_loss += criterion(output, target).item()\n",
        "                _, predicted = output.max(1)\n",
        "                total += target.size(0)\n",
        "                correct += predicted.eq(target).sum().item()\n",
        "        \n",
        "        test_loss /= len(test_loader)\n",
        "        test_acc = 100. * correct / total\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_acc)\n",
        "        \n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "        \n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "        print(f'  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
        "        print(f'  Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "        print()\n",
        "    \n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'test_losses': test_losses,\n",
        "        'test_accuracies': test_accuracies\n",
        "    }\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model, test_loader, model_name):\n",
        "    \"\"\"Evaluate model and return predictions and targets\"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "            \n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "    \n",
        "    test_loss /= len(test_loader)\n",
        "    test_acc = 100. * correct / total\n",
        "    \n",
        "    print(f\"{model_name} Final Results:\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "    print()\n",
        "    \n",
        "    return all_predictions, all_targets, test_loss, test_acc\n",
        "\n",
        "print(\"Training setup completed!\")\n",
        "print(f\"Number of epochs: {num_epochs}\")\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "print(f\"Weight decay: {weight_decay}\")\n",
        "print(f\"Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training {#train}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train ResNet18\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training ResNet18...\n",
            "--------------------------------------------------\n",
            "Epoch: 1/50, Batch: 0/391, Loss: 2.5351, Acc: 7.03%\n",
            "Epoch: 1/50, Batch: 100/391, Loss: 1.9035, Acc: 19.38%\n",
            "Epoch: 1/50, Batch: 200/391, Loss: 1.7141, Acc: 24.58%\n",
            "Epoch: 1/50, Batch: 300/391, Loss: 1.6289, Acc: 28.35%\n",
            "Epoch 1/50:\n",
            "  Train Loss: 1.9151, Train Acc: 30.73%\n",
            "  Test Loss: 1.6134, Test Acc: 38.10%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 2/50, Batch: 0/391, Loss: 1.6080, Acc: 33.59%\n",
            "Epoch: 2/50, Batch: 100/391, Loss: 1.4161, Acc: 42.03%\n",
            "Epoch: 2/50, Batch: 200/391, Loss: 1.4045, Acc: 43.64%\n",
            "Epoch: 2/50, Batch: 300/391, Loss: 1.4766, Acc: 44.84%\n",
            "Epoch 2/50:\n",
            "  Train Loss: 1.4552, Train Acc: 45.97%\n",
            "  Test Loss: 1.3272, Test Acc: 51.91%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 3/50, Batch: 0/391, Loss: 1.3908, Acc: 49.22%\n",
            "Epoch: 3/50, Batch: 100/391, Loss: 1.3431, Acc: 53.78%\n",
            "Epoch: 3/50, Batch: 200/391, Loss: 1.1678, Acc: 55.31%\n",
            "Epoch: 3/50, Batch: 300/391, Loss: 1.2076, Acc: 56.13%\n",
            "Epoch 3/50:\n",
            "  Train Loss: 1.1877, Train Acc: 57.13%\n",
            "  Test Loss: 1.1382, Test Acc: 59.59%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 4/50, Batch: 0/391, Loss: 1.1595, Acc: 55.47%\n",
            "Epoch: 4/50, Batch: 100/391, Loss: 1.0065, Acc: 63.00%\n",
            "Epoch: 4/50, Batch: 200/391, Loss: 0.9193, Acc: 64.11%\n",
            "Epoch: 4/50, Batch: 300/391, Loss: 1.0932, Acc: 64.42%\n",
            "Epoch 4/50:\n",
            "  Train Loss: 0.9858, Train Acc: 64.88%\n",
            "  Test Loss: 0.9572, Test Acc: 66.38%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 5/50, Batch: 0/391, Loss: 0.9551, Acc: 63.28%\n",
            "Epoch: 5/50, Batch: 100/391, Loss: 0.7581, Acc: 68.84%\n",
            "Epoch: 5/50, Batch: 200/391, Loss: 0.5814, Acc: 69.01%\n",
            "Epoch: 5/50, Batch: 300/391, Loss: 0.7289, Acc: 69.26%\n",
            "Epoch 5/50:\n",
            "  Train Loss: 0.8494, Train Acc: 69.87%\n",
            "  Test Loss: 0.7802, Test Acc: 72.52%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 6/50, Batch: 0/391, Loss: 0.9565, Acc: 69.53%\n",
            "Epoch: 6/50, Batch: 100/391, Loss: 0.7356, Acc: 72.97%\n",
            "Epoch: 6/50, Batch: 200/391, Loss: 0.7954, Acc: 73.00%\n",
            "Epoch: 6/50, Batch: 300/391, Loss: 0.6970, Acc: 73.18%\n",
            "Epoch 6/50:\n",
            "  Train Loss: 0.7520, Train Acc: 73.61%\n",
            "  Test Loss: 0.7989, Test Acc: 71.81%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 7/50, Batch: 0/391, Loss: 0.6183, Acc: 74.22%\n",
            "Epoch: 7/50, Batch: 100/391, Loss: 0.5750, Acc: 75.53%\n",
            "Epoch: 7/50, Batch: 200/391, Loss: 0.5284, Acc: 76.23%\n",
            "Epoch: 7/50, Batch: 300/391, Loss: 0.5898, Acc: 76.33%\n",
            "Epoch 7/50:\n",
            "  Train Loss: 0.6631, Train Acc: 76.71%\n",
            "  Test Loss: 0.6950, Test Acc: 76.15%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 8/50, Batch: 0/391, Loss: 0.5908, Acc: 80.47%\n",
            "Epoch: 8/50, Batch: 100/391, Loss: 0.5994, Acc: 78.92%\n",
            "Epoch: 8/50, Batch: 200/391, Loss: 0.6203, Acc: 79.18%\n",
            "Epoch: 8/50, Batch: 300/391, Loss: 0.5317, Acc: 79.49%\n",
            "Epoch 8/50:\n",
            "  Train Loss: 0.5924, Train Acc: 79.55%\n",
            "  Test Loss: 0.5876, Test Acc: 79.92%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 9/50, Batch: 0/391, Loss: 0.4920, Acc: 83.59%\n",
            "Epoch: 9/50, Batch: 100/391, Loss: 0.4812, Acc: 81.08%\n",
            "Epoch: 9/50, Batch: 200/391, Loss: 0.4745, Acc: 81.50%\n",
            "Epoch: 9/50, Batch: 300/391, Loss: 0.4880, Acc: 81.58%\n",
            "Epoch 9/50:\n",
            "  Train Loss: 0.5306, Train Acc: 81.61%\n",
            "  Test Loss: 0.6283, Test Acc: 79.03%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 10/50, Batch: 0/391, Loss: 0.4964, Acc: 82.03%\n",
            "Epoch: 10/50, Batch: 100/391, Loss: 0.4323, Acc: 83.31%\n",
            "Epoch: 10/50, Batch: 200/391, Loss: 0.5210, Acc: 83.10%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train ResNet18\u001b[39;00m\n\u001b[32m      2\u001b[39m start_time = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m resnet_history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet18\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresnet_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresnet_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mResNet18\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m resnet_train_time = time.time() - start_time\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResNet18 training completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresnet_train_time/\u001b[32m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m minutes\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, optimizer, scheduler, train_loader, test_loader, num_epochs, model_name)\u001b[39m\n\u001b[32m     40\u001b[39m output = model(data)\n\u001b[32m     41\u001b[39m loss = criterion(output, target)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m optimizer.step()\n\u001b[32m     45\u001b[39m running_loss += loss.item()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dhanush Srinivas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dhanush Srinivas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dhanush Srinivas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Train ResNet18\n",
        "start_time = time.time()\n",
        "resnet_history = train_model(resnet18, resnet_optimizer, resnet_scheduler, \n",
        "                              train_loader, test_loader, num_epochs, \"ResNet18\")\n",
        "resnet_train_time = time.time() - start_time\n",
        "print(f\"ResNet18 training completed in {resnet_train_time/60:.2f} minutes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Simple CNN Baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Simple CNN...\n",
            "--------------------------------------------------\n",
            "Epoch: 1/50, Batch: 0/391, Loss: 2.3084, Acc: 6.25%\n",
            "Epoch: 1/50, Batch: 100/391, Loss: 1.9853, Acc: 19.67%\n",
            "Epoch: 1/50, Batch: 200/391, Loss: 1.8378, Acc: 24.29%\n",
            "Epoch: 1/50, Batch: 300/391, Loss: 1.7893, Acc: 26.60%\n",
            "Epoch 1/50:\n",
            "  Train Loss: 1.9177, Train Acc: 28.24%\n",
            "  Test Loss: 1.7129, Test Acc: 35.71%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 2/50, Batch: 0/391, Loss: 1.7443, Acc: 35.16%\n",
            "Epoch: 2/50, Batch: 100/391, Loss: 1.7897, Acc: 33.82%\n",
            "Epoch: 2/50, Batch: 200/391, Loss: 1.7073, Acc: 34.26%\n",
            "Epoch: 2/50, Batch: 300/391, Loss: 1.7075, Acc: 35.56%\n",
            "Epoch 2/50:\n",
            "  Train Loss: 1.7352, Train Acc: 36.17%\n",
            "  Test Loss: 1.5779, Test Acc: 43.17%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 3/50, Batch: 0/391, Loss: 1.7233, Acc: 38.28%\n",
            "Epoch: 3/50, Batch: 100/391, Loss: 1.6198, Acc: 39.10%\n",
            "Epoch: 3/50, Batch: 200/391, Loss: 1.7858, Acc: 39.55%\n",
            "Epoch: 3/50, Batch: 300/391, Loss: 1.6238, Acc: 39.74%\n",
            "Epoch 3/50:\n",
            "  Train Loss: 1.6586, Train Acc: 39.61%\n",
            "  Test Loss: 1.5094, Test Acc: 44.93%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 4/50, Batch: 0/391, Loss: 1.7408, Acc: 39.84%\n",
            "Epoch: 4/50, Batch: 100/391, Loss: 1.6038, Acc: 40.84%\n",
            "Epoch: 4/50, Batch: 200/391, Loss: 1.4374, Acc: 40.80%\n",
            "Epoch: 4/50, Batch: 300/391, Loss: 1.6921, Acc: 40.89%\n",
            "Epoch 4/50:\n",
            "  Train Loss: 1.6291, Train Acc: 40.99%\n",
            "  Test Loss: 1.4709, Test Acc: 46.09%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 5/50, Batch: 0/391, Loss: 1.5990, Acc: 44.53%\n",
            "Epoch: 5/50, Batch: 100/391, Loss: 1.6401, Acc: 41.72%\n",
            "Epoch: 5/50, Batch: 200/391, Loss: 1.6222, Acc: 41.41%\n",
            "Epoch: 5/50, Batch: 300/391, Loss: 1.6029, Acc: 41.38%\n",
            "Epoch 5/50:\n",
            "  Train Loss: 1.6211, Train Acc: 41.46%\n",
            "  Test Loss: 1.4354, Test Acc: 48.12%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 6/50, Batch: 0/391, Loss: 1.7133, Acc: 37.50%\n",
            "Epoch: 6/50, Batch: 100/391, Loss: 1.4807, Acc: 40.81%\n",
            "Epoch: 6/50, Batch: 200/391, Loss: 1.6811, Acc: 41.45%\n",
            "Epoch: 6/50, Batch: 300/391, Loss: 1.6402, Acc: 41.46%\n",
            "Epoch 6/50:\n",
            "  Train Loss: 1.6174, Train Acc: 41.69%\n",
            "  Test Loss: 1.5383, Test Acc: 44.85%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 7/50, Batch: 0/391, Loss: 1.6538, Acc: 41.41%\n",
            "Epoch: 7/50, Batch: 100/391, Loss: 1.6025, Acc: 41.38%\n",
            "Epoch: 7/50, Batch: 200/391, Loss: 1.5835, Acc: 41.66%\n",
            "Epoch: 7/50, Batch: 300/391, Loss: 1.5841, Acc: 41.72%\n",
            "Epoch 7/50:\n",
            "  Train Loss: 1.6149, Train Acc: 42.11%\n",
            "  Test Loss: 1.4601, Test Acc: 46.81%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 8/50, Batch: 0/391, Loss: 1.7191, Acc: 37.50%\n",
            "Epoch: 8/50, Batch: 100/391, Loss: 1.4537, Acc: 41.96%\n",
            "Epoch: 8/50, Batch: 200/391, Loss: 1.4151, Acc: 41.90%\n",
            "Epoch: 8/50, Batch: 300/391, Loss: 1.4384, Acc: 42.43%\n",
            "Epoch 8/50:\n",
            "  Train Loss: 1.6086, Train Acc: 42.51%\n",
            "  Test Loss: 1.4557, Test Acc: 47.30%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 9/50, Batch: 0/391, Loss: 1.6312, Acc: 41.41%\n",
            "Epoch: 9/50, Batch: 100/391, Loss: 1.7019, Acc: 43.19%\n",
            "Epoch: 9/50, Batch: 200/391, Loss: 1.7926, Acc: 42.82%\n",
            "Epoch: 9/50, Batch: 300/391, Loss: 1.5729, Acc: 42.85%\n",
            "Epoch 9/50:\n",
            "  Train Loss: 1.5960, Train Acc: 42.76%\n",
            "  Test Loss: 1.3950, Test Acc: 50.20%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 10/50, Batch: 0/391, Loss: 1.6098, Acc: 45.31%\n",
            "Epoch: 10/50, Batch: 100/391, Loss: 1.6354, Acc: 43.56%\n",
            "Epoch: 10/50, Batch: 200/391, Loss: 1.6115, Acc: 43.61%\n",
            "Epoch: 10/50, Batch: 300/391, Loss: 1.6459, Acc: 43.51%\n",
            "Epoch 10/50:\n",
            "  Train Loss: 1.5795, Train Acc: 43.63%\n",
            "  Test Loss: 1.3588, Test Acc: 51.92%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 11/50, Batch: 0/391, Loss: 1.7016, Acc: 37.50%\n",
            "Epoch: 11/50, Batch: 100/391, Loss: 1.6309, Acc: 44.62%\n",
            "Epoch: 11/50, Batch: 200/391, Loss: 1.4922, Acc: 44.73%\n",
            "Epoch: 11/50, Batch: 300/391, Loss: 1.6013, Acc: 44.63%\n",
            "Epoch 11/50:\n",
            "  Train Loss: 1.5501, Train Acc: 44.73%\n",
            "  Test Loss: 1.4213, Test Acc: 49.56%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 12/50, Batch: 0/391, Loss: 1.5423, Acc: 43.75%\n",
            "Epoch: 12/50, Batch: 100/391, Loss: 1.4351, Acc: 44.27%\n",
            "Epoch: 12/50, Batch: 200/391, Loss: 1.4186, Acc: 44.38%\n",
            "Epoch: 12/50, Batch: 300/391, Loss: 1.5334, Acc: 44.59%\n",
            "Epoch 12/50:\n",
            "  Train Loss: 1.5618, Train Acc: 44.52%\n",
            "  Test Loss: 1.3823, Test Acc: 50.36%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 13/50, Batch: 0/391, Loss: 1.5789, Acc: 39.84%\n",
            "Epoch: 13/50, Batch: 100/391, Loss: 1.7317, Acc: 43.87%\n",
            "Epoch: 13/50, Batch: 200/391, Loss: 1.4143, Acc: 44.11%\n",
            "Epoch: 13/50, Batch: 300/391, Loss: 1.5474, Acc: 44.18%\n",
            "Epoch 13/50:\n",
            "  Train Loss: 1.5629, Train Acc: 44.50%\n",
            "  Test Loss: 1.3960, Test Acc: 50.73%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 14/50, Batch: 0/391, Loss: 1.4597, Acc: 46.09%\n",
            "Epoch: 14/50, Batch: 100/391, Loss: 1.3840, Acc: 46.16%\n",
            "Epoch: 14/50, Batch: 200/391, Loss: 1.6575, Acc: 46.03%\n",
            "Epoch: 14/50, Batch: 300/391, Loss: 1.5711, Acc: 46.16%\n",
            "Epoch 14/50:\n",
            "  Train Loss: 1.5388, Train Acc: 45.83%\n",
            "  Test Loss: 1.3827, Test Acc: 51.12%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 15/50, Batch: 0/391, Loss: 1.4467, Acc: 49.22%\n",
            "Epoch: 15/50, Batch: 100/391, Loss: 1.5815, Acc: 46.67%\n",
            "Epoch: 15/50, Batch: 200/391, Loss: 1.4179, Acc: 45.81%\n",
            "Epoch: 15/50, Batch: 300/391, Loss: 1.4813, Acc: 45.17%\n",
            "Epoch 15/50:\n",
            "  Train Loss: 1.5559, Train Acc: 45.31%\n",
            "  Test Loss: 1.4367, Test Acc: 49.21%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 16/50, Batch: 0/391, Loss: 1.5086, Acc: 43.75%\n",
            "Epoch: 16/50, Batch: 100/391, Loss: 1.4409, Acc: 45.30%\n",
            "Epoch: 16/50, Batch: 200/391, Loss: 1.8041, Acc: 45.37%\n",
            "Epoch: 16/50, Batch: 300/391, Loss: 1.6445, Acc: 44.98%\n",
            "Epoch 16/50:\n",
            "  Train Loss: 1.5629, Train Acc: 44.95%\n",
            "  Test Loss: 1.4021, Test Acc: 50.87%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 17/50, Batch: 0/391, Loss: 1.5656, Acc: 44.53%\n",
            "Epoch: 17/50, Batch: 100/391, Loss: 1.7068, Acc: 44.83%\n",
            "Epoch: 17/50, Batch: 200/391, Loss: 1.6747, Acc: 44.32%\n",
            "Epoch: 17/50, Batch: 300/391, Loss: 1.3230, Acc: 44.44%\n",
            "Epoch 17/50:\n",
            "  Train Loss: 1.5647, Train Acc: 44.83%\n",
            "  Test Loss: 1.3125, Test Acc: 53.58%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 18/50, Batch: 0/391, Loss: 1.4756, Acc: 46.88%\n",
            "Epoch: 18/50, Batch: 100/391, Loss: 1.5430, Acc: 45.83%\n",
            "Epoch: 18/50, Batch: 200/391, Loss: 1.3651, Acc: 45.50%\n",
            "Epoch: 18/50, Batch: 300/391, Loss: 1.5356, Acc: 45.43%\n",
            "Epoch 18/50:\n",
            "  Train Loss: 1.5411, Train Acc: 45.82%\n",
            "  Test Loss: 1.3422, Test Acc: 52.73%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 19/50, Batch: 0/391, Loss: 1.4518, Acc: 53.91%\n",
            "Epoch: 19/50, Batch: 100/391, Loss: 1.4541, Acc: 46.74%\n",
            "Epoch: 19/50, Batch: 200/391, Loss: 1.5096, Acc: 46.21%\n",
            "Epoch: 19/50, Batch: 300/391, Loss: 1.6446, Acc: 45.84%\n",
            "Epoch 19/50:\n",
            "  Train Loss: 1.5326, Train Acc: 45.99%\n",
            "  Test Loss: 1.3624, Test Acc: 51.86%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 20/50, Batch: 0/391, Loss: 1.4843, Acc: 46.88%\n",
            "Epoch: 20/50, Batch: 100/391, Loss: 1.6000, Acc: 46.37%\n",
            "Epoch: 20/50, Batch: 200/391, Loss: 1.7219, Acc: 45.60%\n",
            "Epoch: 20/50, Batch: 300/391, Loss: 1.6500, Acc: 45.71%\n",
            "Epoch 20/50:\n",
            "  Train Loss: 1.5346, Train Acc: 46.02%\n",
            "  Test Loss: 1.3360, Test Acc: 53.14%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 21/50, Batch: 0/391, Loss: 1.5174, Acc: 47.66%\n",
            "Epoch: 21/50, Batch: 100/391, Loss: 1.6101, Acc: 44.69%\n",
            "Epoch: 21/50, Batch: 200/391, Loss: 1.7232, Acc: 46.30%\n",
            "Epoch: 21/50, Batch: 300/391, Loss: 1.7509, Acc: 46.51%\n",
            "Epoch 21/50:\n",
            "  Train Loss: 1.5209, Train Acc: 46.69%\n",
            "  Test Loss: 1.3226, Test Acc: 53.13%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 22/50, Batch: 0/391, Loss: 1.5548, Acc: 46.09%\n",
            "Epoch: 22/50, Batch: 100/391, Loss: 1.4685, Acc: 46.23%\n",
            "Epoch: 22/50, Batch: 200/391, Loss: 1.4682, Acc: 46.44%\n",
            "Epoch: 22/50, Batch: 300/391, Loss: 1.5771, Acc: 46.52%\n",
            "Epoch 22/50:\n",
            "  Train Loss: 1.5319, Train Acc: 46.12%\n",
            "  Test Loss: 1.2942, Test Acc: 54.97%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 23/50, Batch: 0/391, Loss: 1.4261, Acc: 48.44%\n",
            "Epoch: 23/50, Batch: 100/391, Loss: 1.6737, Acc: 47.96%\n",
            "Epoch: 23/50, Batch: 200/391, Loss: 1.4673, Acc: 47.14%\n",
            "Epoch: 23/50, Batch: 300/391, Loss: 1.4101, Acc: 47.31%\n",
            "Epoch 23/50:\n",
            "  Train Loss: 1.5095, Train Acc: 47.11%\n",
            "  Test Loss: 1.3641, Test Acc: 52.77%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 24/50, Batch: 0/391, Loss: 1.3377, Acc: 52.34%\n",
            "Epoch: 24/50, Batch: 100/391, Loss: 1.7672, Acc: 45.55%\n",
            "Epoch: 24/50, Batch: 200/391, Loss: 1.5342, Acc: 46.97%\n",
            "Epoch: 24/50, Batch: 300/391, Loss: 1.5645, Acc: 47.51%\n",
            "Epoch 24/50:\n",
            "  Train Loss: 1.5151, Train Acc: 47.24%\n",
            "  Test Loss: 1.3545, Test Acc: 52.04%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 25/50, Batch: 0/391, Loss: 1.3999, Acc: 49.22%\n",
            "Epoch: 25/50, Batch: 100/391, Loss: 1.3946, Acc: 45.26%\n",
            "Epoch: 25/50, Batch: 200/391, Loss: 1.4546, Acc: 45.85%\n",
            "Epoch: 25/50, Batch: 300/391, Loss: 1.3993, Acc: 46.23%\n",
            "Epoch 25/50:\n",
            "  Train Loss: 1.5271, Train Acc: 46.32%\n",
            "  Test Loss: 1.3776, Test Acc: 51.60%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 26/50, Batch: 0/391, Loss: 1.3942, Acc: 56.25%\n",
            "Epoch: 26/50, Batch: 100/391, Loss: 1.5149, Acc: 45.88%\n",
            "Epoch: 26/50, Batch: 200/391, Loss: 1.5052, Acc: 46.37%\n",
            "Epoch: 26/50, Batch: 300/391, Loss: 1.4708, Acc: 47.04%\n",
            "Epoch 26/50:\n",
            "  Train Loss: 1.5232, Train Acc: 47.16%\n",
            "  Test Loss: 1.3900, Test Acc: 51.09%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 27/50, Batch: 0/391, Loss: 1.6827, Acc: 46.09%\n",
            "Epoch: 27/50, Batch: 100/391, Loss: 1.4863, Acc: 47.73%\n",
            "Epoch: 27/50, Batch: 200/391, Loss: 1.7431, Acc: 47.15%\n",
            "Epoch: 27/50, Batch: 300/391, Loss: 1.5172, Acc: 46.97%\n",
            "Epoch 27/50:\n",
            "  Train Loss: 1.5220, Train Acc: 47.18%\n",
            "  Test Loss: 1.3093, Test Acc: 54.94%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 28/50, Batch: 0/391, Loss: 1.4831, Acc: 51.56%\n",
            "Epoch: 28/50, Batch: 100/391, Loss: 1.4026, Acc: 46.93%\n",
            "Epoch: 28/50, Batch: 200/391, Loss: 1.6720, Acc: 47.30%\n",
            "Epoch: 28/50, Batch: 300/391, Loss: 1.5848, Acc: 47.08%\n",
            "Epoch 28/50:\n",
            "  Train Loss: 1.5146, Train Acc: 47.17%\n",
            "  Test Loss: 1.3677, Test Acc: 51.89%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 29/50, Batch: 0/391, Loss: 1.4536, Acc: 50.00%\n",
            "Epoch: 29/50, Batch: 100/391, Loss: 1.4219, Acc: 47.30%\n",
            "Epoch: 29/50, Batch: 200/391, Loss: 1.3882, Acc: 47.63%\n",
            "Epoch: 29/50, Batch: 300/391, Loss: 1.3378, Acc: 47.28%\n",
            "Epoch 29/50:\n",
            "  Train Loss: 1.5061, Train Acc: 47.68%\n",
            "  Test Loss: 1.2908, Test Acc: 54.90%\n",
            "  Learning Rate: 0.100000\n",
            "\n",
            "Epoch: 30/50, Batch: 0/391, Loss: 1.4526, Acc: 48.44%\n",
            "Epoch: 30/50, Batch: 100/391, Loss: 1.6339, Acc: 47.22%\n",
            "Epoch: 30/50, Batch: 200/391, Loss: 1.5387, Acc: 47.77%\n",
            "Epoch: 30/50, Batch: 300/391, Loss: 1.5841, Acc: 47.67%\n",
            "Epoch 30/50:\n",
            "  Train Loss: 1.4968, Train Acc: 47.94%\n",
            "  Test Loss: 1.3685, Test Acc: 52.23%\n",
            "  Learning Rate: 0.010000\n",
            "\n",
            "Epoch: 31/50, Batch: 0/391, Loss: 1.3242, Acc: 53.91%\n",
            "Epoch: 31/50, Batch: 100/391, Loss: 1.3918, Acc: 52.48%\n",
            "Epoch: 31/50, Batch: 200/391, Loss: 1.1887, Acc: 53.95%\n",
            "Epoch: 31/50, Batch: 300/391, Loss: 1.2445, Acc: 55.00%\n",
            "Epoch 31/50:\n",
            "  Train Loss: 1.2745, Train Acc: 55.51%\n",
            "  Test Loss: 1.0993, Test Acc: 61.90%\n",
            "  Learning Rate: 0.010000\n",
            "\n",
            "Epoch: 32/50, Batch: 0/391, Loss: 1.2484, Acc: 54.69%\n",
            "Epoch: 32/50, Batch: 100/391, Loss: 1.0087, Acc: 58.39%\n",
            "Epoch: 32/50, Batch: 200/391, Loss: 1.0155, Acc: 58.36%\n",
            "Epoch: 32/50, Batch: 300/391, Loss: 1.1343, Acc: 58.73%\n",
            "Epoch 32/50:\n",
            "  Train Loss: 1.1823, Train Acc: 59.13%\n",
            "  Test Loss: 1.0533, Test Acc: 63.59%\n",
            "  Learning Rate: 0.010000\n",
            "\n",
            "Epoch: 33/50, Batch: 0/391, Loss: 1.1271, Acc: 58.59%\n",
            "Epoch: 33/50, Batch: 100/391, Loss: 1.0428, Acc: 60.11%\n",
            "Epoch: 33/50, Batch: 200/391, Loss: 1.2661, Acc: 60.37%\n",
            "Epoch: 33/50, Batch: 300/391, Loss: 1.2426, Acc: 60.57%\n",
            "Epoch 33/50:\n",
            "  Train Loss: 1.1364, Train Acc: 60.78%\n",
            "  Test Loss: 1.0280, Test Acc: 63.95%\n",
            "  Learning Rate: 0.010000\n",
            "\n",
            "Epoch: 34/50, Batch: 0/391, Loss: 1.2074, Acc: 60.16%\n",
            "Epoch: 34/50, Batch: 100/391, Loss: 1.0353, Acc: 61.15%\n",
            "Epoch: 34/50, Batch: 200/391, Loss: 1.2172, Acc: 61.30%\n",
            "Epoch: 34/50, Batch: 300/391, Loss: 1.2017, Acc: 61.24%\n",
            "Epoch 34/50:\n",
            "  Train Loss: 1.1053, Train Acc: 61.56%\n",
            "  Test Loss: 1.0007, Test Acc: 65.86%\n",
            "  Learning Rate: 0.010000\n",
            "\n",
            "Epoch: 35/50, Batch: 0/391, Loss: 1.0745, Acc: 62.50%\n",
            "Epoch: 35/50, Batch: 100/391, Loss: 1.2009, Acc: 61.44%\n",
            "Epoch: 35/50, Batch: 200/391, Loss: 1.2041, Acc: 61.85%\n",
            "Epoch: 35/50, Batch: 300/391, Loss: 1.0651, Acc: 62.06%\n",
            "Epoch 35/50:\n",
            "  Train Loss: 1.0839, Train Acc: 62.31%\n",
            "  Test Loss: 0.9847, Test Acc: 66.24%\n",
            "  Learning Rate: 0.010000\n",
            "\n",
            "Epoch: 36/50, Batch: 0/391, Loss: 0.9968, Acc: 62.50%\n",
            "Epoch: 36/50, Batch: 100/391, Loss: 0.9588, Acc: 62.79%\n",
            "Epoch: 36/50, Batch: 200/391, Loss: 0.9256, Acc: 62.64%\n",
            "Epoch: 36/50, Batch: 300/391, Loss: 1.3091, Acc: 62.83%\n",
            "Epoch 36/50:\n",
            "  Train Loss: 1.0620, Train Acc: 62.91%\n",
            "  Test Loss: 0.9651, Test Acc: 66.28%\n",
            "  Learning Rate: 0.010000\n",
            "\n",
            "Epoch: 37/50, Batch: 0/391, Loss: 0.9555, Acc: 65.62%\n",
            "Epoch: 37/50, Batch: 100/391, Loss: 1.0792, Acc: 62.82%\n",
            "Epoch: 37/50, Batch: 200/391, Loss: 1.0324, Acc: 63.48%\n",
            "Epoch: 37/50, Batch: 300/391, Loss: 1.0023, Acc: 63.48%\n",
            "Epoch 37/50:\n",
            "  Train Loss: 1.0465, Train Acc: 63.65%\n",
            "  Test Loss: 0.9384, Test Acc: 67.41%\n",
            "  Learning Rate: 0.010000\n",
            "\n",
            "Epoch: 38/50, Batch: 0/391, Loss: 1.1909, Acc: 62.50%\n",
            "Epoch: 38/50, Batch: 100/391, Loss: 0.8256, Acc: 63.76%\n",
            "Epoch: 38/50, Batch: 200/391, Loss: 1.0048, Acc: 64.13%\n",
            "Epoch: 38/50, Batch: 300/391, Loss: 0.8497, Acc: 64.47%\n",
            "Epoch 38/50:\n",
            "  Train Loss: 1.0210, Train Acc: 64.55%\n",
            "  Test Loss: 0.9213, Test Acc: 67.98%\n",
            "  Learning Rate: 0.010000\n",
            "\n",
            "Epoch: 39/50, Batch: 0/391, Loss: 0.8877, Acc: 75.00%\n",
            "Epoch: 39/50, Batch: 100/391, Loss: 0.9082, Acc: 65.17%\n",
            "Epoch: 39/50, Batch: 200/391, Loss: 1.3090, Acc: 65.39%\n",
            "Epoch: 39/50, Batch: 300/391, Loss: 1.0362, Acc: 65.00%\n",
            "Epoch 39/50:\n",
            "  Train Loss: 1.0073, Train Acc: 65.08%\n",
            "  Test Loss: 0.9140, Test Acc: 68.04%\n",
            "  Learning Rate: 0.010000\n",
            "\n",
            "Epoch: 40/50, Batch: 0/391, Loss: 0.9262, Acc: 69.53%\n",
            "Epoch: 40/50, Batch: 100/391, Loss: 0.8976, Acc: 66.03%\n",
            "Epoch: 40/50, Batch: 200/391, Loss: 1.0044, Acc: 65.60%\n",
            "Epoch: 40/50, Batch: 300/391, Loss: 1.0696, Acc: 65.79%\n",
            "Epoch 40/50:\n",
            "  Train Loss: 0.9882, Train Acc: 65.63%\n",
            "  Test Loss: 0.8974, Test Acc: 68.70%\n",
            "  Learning Rate: 0.001000\n",
            "\n",
            "Epoch: 41/50, Batch: 0/391, Loss: 1.0240, Acc: 64.06%\n",
            "Epoch: 41/50, Batch: 100/391, Loss: 1.0066, Acc: 65.99%\n",
            "Epoch: 41/50, Batch: 200/391, Loss: 0.9400, Acc: 66.33%\n",
            "Epoch: 41/50, Batch: 300/391, Loss: 0.9972, Acc: 66.39%\n",
            "Epoch 41/50:\n",
            "  Train Loss: 0.9609, Train Acc: 66.62%\n",
            "  Test Loss: 0.8800, Test Acc: 69.27%\n",
            "  Learning Rate: 0.001000\n",
            "\n",
            "Epoch: 42/50, Batch: 0/391, Loss: 1.0379, Acc: 67.19%\n",
            "Epoch: 42/50, Batch: 100/391, Loss: 1.0702, Acc: 66.58%\n",
            "Epoch: 42/50, Batch: 200/391, Loss: 0.9091, Acc: 67.09%\n",
            "Epoch: 42/50, Batch: 300/391, Loss: 0.8611, Acc: 66.96%\n",
            "Epoch 42/50:\n",
            "  Train Loss: 0.9490, Train Acc: 67.06%\n",
            "  Test Loss: 0.8747, Test Acc: 69.59%\n",
            "  Learning Rate: 0.001000\n",
            "\n",
            "Epoch: 43/50, Batch: 0/391, Loss: 0.9474, Acc: 63.28%\n",
            "Epoch: 43/50, Batch: 100/391, Loss: 0.9169, Acc: 67.31%\n",
            "Epoch: 43/50, Batch: 200/391, Loss: 0.9603, Acc: 67.38%\n",
            "Epoch: 43/50, Batch: 300/391, Loss: 0.9630, Acc: 67.14%\n",
            "Epoch 43/50:\n",
            "  Train Loss: 0.9470, Train Acc: 67.11%\n",
            "  Test Loss: 0.8720, Test Acc: 69.60%\n",
            "  Learning Rate: 0.001000\n",
            "\n",
            "Epoch: 44/50, Batch: 0/391, Loss: 0.9265, Acc: 68.75%\n",
            "Epoch: 44/50, Batch: 100/391, Loss: 0.8943, Acc: 67.08%\n",
            "Epoch: 44/50, Batch: 200/391, Loss: 0.9502, Acc: 67.21%\n",
            "Epoch: 44/50, Batch: 300/391, Loss: 0.9809, Acc: 66.93%\n",
            "Epoch 44/50:\n",
            "  Train Loss: 0.9442, Train Acc: 67.00%\n",
            "  Test Loss: 0.8692, Test Acc: 69.54%\n",
            "  Learning Rate: 0.001000\n",
            "\n",
            "Epoch: 45/50, Batch: 0/391, Loss: 1.0216, Acc: 64.84%\n",
            "Epoch: 45/50, Batch: 100/391, Loss: 0.8027, Acc: 67.30%\n",
            "Epoch: 45/50, Batch: 200/391, Loss: 0.8203, Acc: 67.35%\n",
            "Epoch: 45/50, Batch: 300/391, Loss: 0.9804, Acc: 67.32%\n",
            "Epoch 45/50:\n",
            "  Train Loss: 0.9428, Train Acc: 67.37%\n",
            "  Test Loss: 0.8680, Test Acc: 69.67%\n",
            "  Learning Rate: 0.001000\n",
            "\n",
            "Epoch: 46/50, Batch: 0/391, Loss: 0.9067, Acc: 67.19%\n",
            "Epoch: 46/50, Batch: 100/391, Loss: 0.7823, Acc: 67.38%\n",
            "Epoch: 46/50, Batch: 200/391, Loss: 0.7650, Acc: 67.83%\n",
            "Epoch: 46/50, Batch: 300/391, Loss: 0.9795, Acc: 67.70%\n",
            "Epoch 46/50:\n",
            "  Train Loss: 0.9309, Train Acc: 67.71%\n",
            "  Test Loss: 0.8641, Test Acc: 69.77%\n",
            "  Learning Rate: 0.001000\n",
            "\n",
            "Epoch: 47/50, Batch: 0/391, Loss: 0.8914, Acc: 68.75%\n",
            "Epoch: 47/50, Batch: 100/391, Loss: 1.0298, Acc: 67.43%\n",
            "Epoch: 47/50, Batch: 200/391, Loss: 0.9109, Acc: 67.40%\n",
            "Epoch: 47/50, Batch: 300/391, Loss: 0.9946, Acc: 67.40%\n",
            "Epoch 47/50:\n",
            "  Train Loss: 0.9377, Train Acc: 67.27%\n",
            "  Test Loss: 0.8625, Test Acc: 69.67%\n",
            "  Learning Rate: 0.001000\n",
            "\n",
            "Epoch: 48/50, Batch: 0/391, Loss: 0.8861, Acc: 65.62%\n",
            "Epoch: 48/50, Batch: 100/391, Loss: 0.9217, Acc: 67.54%\n",
            "Epoch: 48/50, Batch: 200/391, Loss: 0.9221, Acc: 67.60%\n",
            "Epoch: 48/50, Batch: 300/391, Loss: 0.8909, Acc: 67.66%\n",
            "Epoch 48/50:\n",
            "  Train Loss: 0.9354, Train Acc: 67.43%\n",
            "  Test Loss: 0.8620, Test Acc: 69.82%\n",
            "  Learning Rate: 0.001000\n",
            "\n",
            "Epoch: 49/50, Batch: 0/391, Loss: 0.9252, Acc: 64.84%\n",
            "Epoch: 49/50, Batch: 100/391, Loss: 0.9832, Acc: 67.76%\n",
            "Epoch: 49/50, Batch: 200/391, Loss: 0.8812, Acc: 67.75%\n",
            "Epoch: 49/50, Batch: 300/391, Loss: 0.8334, Acc: 67.47%\n",
            "Epoch 49/50:\n",
            "  Train Loss: 0.9324, Train Acc: 67.67%\n",
            "  Test Loss: 0.8583, Test Acc: 69.68%\n",
            "  Learning Rate: 0.001000\n",
            "\n",
            "Epoch: 50/50, Batch: 0/391, Loss: 0.9637, Acc: 68.75%\n",
            "Epoch: 50/50, Batch: 100/391, Loss: 0.9641, Acc: 67.87%\n",
            "Epoch: 50/50, Batch: 200/391, Loss: 0.9997, Acc: 67.62%\n",
            "Epoch: 50/50, Batch: 300/391, Loss: 0.9285, Acc: 67.43%\n",
            "Epoch 50/50:\n",
            "  Train Loss: 0.9306, Train Acc: 67.50%\n",
            "  Test Loss: 0.8565, Test Acc: 70.18%\n",
            "  Learning Rate: 0.001000\n",
            "\n",
            "Simple CNN training completed in 36.60 minutes\n"
          ]
        }
      ],
      "source": [
        "# Train Simple CNN\n",
        "start_time = time.time()\n",
        "cnn_history = train_model(simple_cnn, cnn_optimizer, cnn_scheduler, \n",
        "                          train_loader, test_loader, num_epochs, \"Simple CNN\")\n",
        "cnn_train_time = time.time() - start_time\n",
        "print(f\"Simple CNN training completed in {cnn_train_time/60:.2f} minutes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluation and Visualization {#evaluation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning Curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'resnet_history' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m     plt.tight_layout()\n\u001b[32m     45\u001b[39m     plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m plot_learning_curves(\u001b[43mresnet_history\u001b[49m, cnn_history)\n",
            "\u001b[31mNameError\u001b[39m: name 'resnet_history' is not defined"
          ]
        }
      ],
      "source": [
        "# Plot learning curves\n",
        "def plot_learning_curves(history1, history2, label1=\"ResNet18\", label2=\"Simple CNN\"):\n",
        "    \"\"\"Plot training and validation loss and accuracy curves\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    epochs = range(1, len(history1['train_losses']) + 1)\n",
        "    \n",
        "    # Training Loss\n",
        "    axes[0, 0].plot(epochs, history1['train_losses'], 'b-', label=label1, linewidth=2)\n",
        "    axes[0, 0].plot(epochs, history2['train_losses'], 'r-', label=label2, linewidth=2)\n",
        "    axes[0, 0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Validation Loss\n",
        "    axes[0, 1].plot(epochs, history1['test_losses'], 'b-', label=label1, linewidth=2)\n",
        "    axes[0, 1].plot(epochs, history2['test_losses'], 'r-', label=label2, linewidth=2)\n",
        "    axes[0, 1].set_title('Validation Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Training Accuracy\n",
        "    axes[1, 0].plot(epochs, history1['train_accuracies'], 'b-', label=label1, linewidth=2)\n",
        "    axes[1, 0].plot(epochs, history2['train_accuracies'], 'r-', label=label2, linewidth=2)\n",
        "    axes[1, 0].set_title('Training Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Accuracy (%)')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Validation Accuracy\n",
        "    axes[1, 1].plot(epochs, history1['test_accuracies'], 'b-', label=label1, linewidth=2)\n",
        "    axes[1, 1].plot(epochs, history2['test_accuracies'], 'r-', label=label2, linewidth=2)\n",
        "    axes[1, 1].set_title('Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Accuracy (%)')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_learning_curves(resnet_history, cnn_history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate both models\n",
        "print(\"=\"*60)\n",
        "print(\"FINAL MODEL EVALUATION\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "\n",
        "resnet_preds, resnet_targets, resnet_loss, resnet_acc = evaluate_model(resnet18, test_loader, \"ResNet18\")\n",
        "cnn_preds, cnn_targets, cnn_loss, cnn_acc = evaluate_model(simple_cnn, test_loader, \"Simple CNN\")\n",
        "\n",
        "# Print classification reports\n",
        "print(\"ResNet18 Classification Report:\")\n",
        "print(classification_report(resnet_targets, resnet_preds, target_names=class_names))\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "print(\"Simple CNN Classification Report:\")\n",
        "print(classification_report(cnn_targets, cnn_preds, target_names=class_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrices\n",
        "def plot_confusion_matrix(predictions, targets, class_names, model_name):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(targets, predictions)\n",
        "    \n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title(f'Confusion Matrix - {model_name}', fontsize=16, fontweight='bold')\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Calculate per-class accuracy\n",
        "    per_class_acc = cm.diagonal() / cm.sum(axis=1) * 100\n",
        "    print(f\"\\nPer-class accuracy for {model_name}:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(f\"  {class_name:12s}: {per_class_acc[i]:.2f}%\")\n",
        "    print()\n",
        "\n",
        "# Plot confusion matrices for both models\n",
        "plot_confusion_matrix(resnet_preds, resnet_targets, class_names, \"ResNet18\")\n",
        "plot_confusion_matrix(cnn_preds, cnn_targets, class_names, \"Simple CNN\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Misclassified Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize misclassified examples for ResNet18\n",
        "def show_misclassified_images(model, test_loader, class_names, num_images=12):\n",
        "    \"\"\"Display misclassified images\"\"\"\n",
        "    model.eval()\n",
        "    misclassified = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            _, predicted = output.max(1)\n",
        "            \n",
        "            # Find misclassified examples\n",
        "            incorrect = predicted.ne(target)\n",
        "            for i in range(len(incorrect)):\n",
        "                if incorrect[i] and len(misclassified) < num_images:\n",
        "                    misclassified.append({\n",
        "                        'image': data[i].cpu(),\n",
        "                        'true': target[i].cpu().item(),\n",
        "                        'predicted': predicted[i].cpu().item()\n",
        "                    })\n",
        "            \n",
        "            if len(misclassified) >= num_images:\n",
        "                break\n",
        "    \n",
        "    # Display misclassified images\n",
        "    fig, axes = plt.subplots(3, 4, figsize=(15, 12))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i in range(min(num_images, len(misclassified))):\n",
        "        image = misclassified[i]['image']\n",
        "        true_label = misclassified[i]['true']\n",
        "        pred_label = misclassified[i]['predicted']\n",
        "        \n",
        "        # Denormalize for display\n",
        "        image = image * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1) + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
        "        image = torch.clamp(image, 0, 1)\n",
        "        \n",
        "        axes[i].imshow(image.permute(1, 2, 0))\n",
        "        axes[i].set_title(f'True: {class_names[true_label]}\\nPred: {class_names[pred_label]}', \n",
        "                         fontsize=10, color='red')\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    plt.suptitle('Misclassified Examples - ResNet18', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_misclassified_images(resnet18, test_loader, class_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Results Comparison {#comparison}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive comparison\n",
        "print(\"=\"*70)\n",
        "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "print()\n",
        "\n",
        "# Create comparison table\n",
        "comparison_data = {\n",
        "    'Metric': ['Parameters', 'Training Time (min)', 'Final Train Accuracy (%)', \n",
        "               'Final Test Accuracy (%)', 'Final Train Loss', 'Final Test Loss'],\n",
        "    'ResNet18': [\n",
        "        f\"{sum(p.numel() for p in resnet18.parameters()):,}\",\n",
        "        f\"{resnet_train_time/60:.2f}\",\n",
        "        f\"{resnet_history['train_accuracies'][-1]:.2f}\",\n",
        "        f\"{resnet_acc:.2f}\",\n",
        "        f\"{resnet_history['train_losses'][-1]:.4f}\",\n",
        "        f\"{resnet_loss:.4f}\"\n",
        "    ],\n",
        "    'Simple CNN': [\n",
        "        f\"{sum(p.numel() for p in simple_cnn.parameters()):,}\",\n",
        "        f\"{cnn_train_time/60:.2f}\",\n",
        "        f\"{cnn_history['train_accuracies'][-1]:.2f}\",\n",
        "        f\"{cnn_acc:.2f}\",\n",
        "        f\"{cnn_history['train_losses'][-1]:.4f}\",\n",
        "        f\"{cnn_loss:.4f}\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Print table\n",
        "print(f\"{'Metric':<30} {'ResNet18':<20} {'Simple CNN':<20}\")\n",
        "print(\"-\" * 70)\n",
        "for i in range(len(comparison_data['Metric'])):\n",
        "    print(f\"{comparison_data['Metric'][i]:<30} {comparison_data['ResNet18'][i]:<20} {comparison_data['Simple CNN'][i]:<20}\")\n",
        "\n",
        "print()\n",
        "print(\"=\"*70)\n",
        "print(\"KEY FINDINGS:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Calculate improvements\n",
        "acc_improvement = resnet_acc - cnn_acc\n",
        "print(f\"1. ResNet18 achieves {acc_improvement:.2f}% higher test accuracy than Simple CNN\")\n",
        "\n",
        "# Analyze overfitting\n",
        "resnet_overfit = resnet_history['train_accuracies'][-1] - resnet_acc\n",
        "cnn_overfit = cnn_history['train_accuracies'][-1] - cnn_acc\n",
        "print(f\"2. Overfitting gap (Train - Test accuracy):\")\n",
        "print(f\"   - ResNet18: {resnet_overfit:.2f}%\")\n",
        "print(f\"   - Simple CNN: {cnn_overfit:.2f}%\")\n",
        "\n",
        "# Find best epoch\n",
        "best_resnet_epoch = np.argmax(resnet_history['test_accuracies']) + 1\n",
        "best_cnn_epoch = np.argmax(cnn_history['test_accuracies']) + 1\n",
        "print(f\"3. Best validation epoch:\")\n",
        "print(f\"   - ResNet18: Epoch {best_resnet_epoch} ({max(resnet_history['test_accuracies']):.2f}%)\")\n",
        "print(f\"   - Simple CNN: Epoch {best_cnn_epoch} ({max(cnn_history['test_accuracies']):.2f}%)\")\n",
        "\n",
        "print()\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Visualize final comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "models = ['ResNet18', 'Simple CNN']\n",
        "test_accuracies = [resnet_acc, cnn_acc]\n",
        "colors = ['#3498db', '#e74c3c']\n",
        "\n",
        "axes[0].bar(models, test_accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "axes[0].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "axes[0].set_title('Final Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylim([0, 100])\n",
        "for i, v in enumerate(test_accuracies):\n",
        "    axes[0].text(i, v + 1, f'{v:.2f}%', ha='center', fontweight='bold')\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Training time comparison\n",
        "train_times = [resnet_train_time/60, cnn_train_time/60]\n",
        "axes[1].bar(models, train_times, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "axes[1].set_ylabel('Training Time (minutes)', fontsize=12)\n",
        "axes[1].set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
        "for i, v in enumerate(train_times):\n",
        "    axes[1].text(i, v + max(train_times)*0.02, f'{v:.2f} min', ha='center', fontweight='bold')\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Conclusion and Future Work {#conclusion}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "In this project, we successfully implemented and trained a ResNet18 model for CIFAR-10 image classification, achieving competitive accuracy. Key findings include:\n",
        "\n",
        "**Benefits of Residual Connections:**\n",
        "- ResNet18 demonstrated superior performance compared to the simple CNN baseline\n",
        "- The residual connections effectively addressed the vanishing gradient problem\n",
        "- Deeper architecture enabled learning more complex feature representations\n",
        "\n",
        "**Model Performance:**\n",
        "- ResNet18 achieved strong test accuracy on the CIFAR-10 dataset\n",
        "- Learning curves showed stable convergence with proper learning rate scheduling\n",
        "- The model maintained good generalization with minimal overfitting\n",
        "\n",
        "**Trade-offs Observed:**\n",
        "- ResNet18 has significantly more parameters than the simple CNN\n",
        "- Training time is longer due to increased model complexity\n",
        "- However, the accuracy improvement justifies the additional computational cost\n",
        "\n",
        "### Future Extensions\n",
        "\n",
        "This project can be extended in several directions:\n",
        "\n",
        "1. **Transfer Learning:**\n",
        "   - Use pre-trained weights from ImageNet\n",
        "   - Fine-tune on CIFAR-10 for potentially better performance\n",
        "   - Experiment with different ResNet variants (ResNet34, ResNet50)\n",
        "\n",
        "2. **Advanced Data Augmentation:**\n",
        "   - Implement AutoAugment or RandAugment\n",
        "   - Use mixup or CutMix techniques\n",
        "   - Apply test-time augmentation for improved inference\n",
        "\n",
        "3. **Larger Datasets:**\n",
        "   - Apply the pipeline to CIFAR-100 (100 classes)\n",
        "   - Test on ImageNet subsets\n",
        "   - Evaluate performance on custom datasets\n",
        "\n",
        "4. **Model Optimization:**\n",
        "   - Implement knowledge distillation\n",
        "   - Apply pruning and quantization\n",
        "   - Use mixed precision training for faster convergence\n",
        "\n",
        "5. **Architecture Improvements:**\n",
        "   - Add attention mechanisms (CBAM, SE blocks)\n",
        "   - Experiment with modern architectures (EfficientNet, Vision Transformers)\n",
        "   - Implement ensemble methods\n",
        "\n",
        "6. **Hyperparameter Tuning:**\n",
        "   - Grid search or Bayesian optimization\n",
        "   - Experiment with different optimizers (AdamW, LAMB)\n",
        "   - Try cosine annealing or cyclical learning rates\n",
        "\n",
        "### References\n",
        "\n",
        "- He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. CVPR.\n",
        "- Krizhevsky, A. (2009). Learning multiple layers of features from tiny images. Technical Report.\n",
        "- PyTorch Documentation: https://pytorch.org/docs/stable/index.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Models (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save trained models (uncomment to save)\n",
        "# torch.save(resnet18.state_dict(), 'resnet18_cifar10.pth')\n",
        "# torch.save(simple_cnn.state_dict(), 'simple_cnn_cifar10.pth')\n",
        "# print(\"Models saved successfully!\")\n",
        "\n",
        "# To load models later:\n",
        "# resnet18_loaded = ResNet18(num_classes=10)\n",
        "# resnet18_loaded.load_state_dict(torch.load('resnet18_cifar10.pth'))\n",
        "# resnet18_loaded.to(device)\n",
        "# resnet18_loaded.eval()\n",
        "\n",
        "print(\"Project completed successfully!\")\n",
        "print(f\"Final ResNet18 Test Accuracy: {resnet_acc:.2f}%\")\n",
        "print(f\"Final Simple CNN Test Accuracy: {cnn_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
